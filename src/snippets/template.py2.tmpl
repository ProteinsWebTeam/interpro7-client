#!/usr/bin/env python2

# standard library modules
import sys, errno, re, json, ssl
import urllib2
from time import sleep

BASE_URL = "<%= url %><%= url.indexOf('?')===-1 ? '?' : '&' %>page_size=200<%= fileType === 'fasta' ? '&extra_fields=sequence' : '' %>"
<% if (fileType === "tsv" && columns.some(x => x.indexOf("extra_fields.counters")>=0)) { 
%>BASE_URL += "&extra_fields=counters"<% 
}
if (fileType === 'fasta') { %>
HEADER_SEPARATOR = "|"
LINE_LENGTH = 80
<% }

if (fileType === "tsv") { %>

def parse_items(items):
  if type(items)==list:
    return ",".join(items)
  return ""
def parse_member_databases(dbs):
  if type(dbs)==dict:
    return ";".join(["{}:{}".format(db, ','.join(dbs[db])) for db in dbs.keys()])
  return ""
def parse_go_terms(gos):
  if type(gos)==list:
    return ",".join([go["identifier"] for go in gos])
  return ""
def parse_locations(locations):
  if type(locations)==list:
    return ",".join(
      [",".join(["{}..{}".format(fragment['start'], fragment['end'])
                for fragment in location["fragments"]
                ])
      for location in locations
      ])
  return ""
def parse_group_column(values, selector):
  return ",".join([parse_column(value, selector) for value in values])

def parse_column(value, selector):
  if value is None:
    return ""
  elif "member_databases" in selector:
    return parse_member_databases(value)
  elif "go_terms" in selector: 
    return parse_go_terms(value)
  elif "children" in selector: 
    return parse_items(value)
  elif "locations" in selector:
    return parse_locations(value)
  return str(value)
<% } %>

def output_list():
  #disable SSL verification to avoid config issues
  context = ssl._create_unverified_context()

  next = BASE_URL
  last_page = False

  <% if (fileType === "json") { %>
  #json header
  sys.stdout.write("{ \"results\": [\n")
  <% } %>
  while next:
    try:
      req = urllib2.Request(next, headers={"Accept": "application/json"})
      res = urllib2.urlopen(req, context=context)
      # If the API times out due a long running query
      if res.getcode() == 408:
        # wait just over a minute
        sleep(61)
        # then continue this loop with the same URL
        continue
      elif res.getcode() == 204:
        #no data so leave loop
        break

    except urllib2.HTTPError as e:
      if e.code == 408:
        sleep(61)
        continue
      else:
        raise e

    payload = json.loads(res.read().decode())
    next = payload["next"]
    if not next:
      last_page = True

    for i, item in enumerate(payload["results"]):
      <% if (fileType === "json" || fileType === "json") { %>
      sys.stdout.write(json.dumps(item))
      # for indented output replace the above line with the following
      # sys.stdout.write(json.dumps(item, indent=4))

      if last_page and i+1 == len(payload["results"]):
        sys.stdout.write("")
      else:
        sys.stdout.write(",\n")
      <% } else if (fileType === "tsv") { 
           for (column of columns) {
             if (column.indexOf("[*]") !== -1) {
               %>sys.stdout.write(parse_group_column(<%= path2code(column, "item") %>, '<%= column %>') + "\t")
      <% 
             } else {
               %>sys.stdout.write(parse_column(<%= path2code(column, "item") %>, '<%= column %>') + "\t")
      <%     } 
           }%>sys.stdout.write("\n")
        <% } else if (fileType === "fasta") { %>
      entries = None
      if ("entry_subset" in item):
        entries = item["entry_subset"]
      elif ("entries" in item):
        entries = item["entries"]
      
      if entries is not None:
        entries_header = ""
        for entry in entries:
          entries_header += " " + entry["accession"] + "(" + ";".join(
            [
              ",".join(
                [ str(fragment["start"]) + "..." + str(fragment["end"]) 
                  for fragment in locations["fragments"]]
              ) for locations in entry["entry_protein_locations"]
            ]
          ) + ")"
        sys.stdout.write(">" + item["metadata"]["accession"] + HEADER_SEPARATOR
                          + entries_header + HEADER_SEPARATOR
                          + item["metadata"]["name"] + "\n")
      else:
        sys.stdout.write(">" + item["metadata"]["accession"] + HEADER_SEPARATOR + item["metadata"]["name"] + "\n")

      seq = item["extra_fields"]["sequence"]
      fastaSeqFragments = [seq[0+i:LINE_LENGTH+i] for i in range(0, len(seq), LINE_LENGTH)]
      for fastaSeqFragment in fastaSeqFragments:
        sys.stdout.write(fastaSeqFragment + "\n")
      <% } else { 
        %>sys.stdout.write(item["metadata"]["accession"] + "\n")<% 
      } %>
    # Don't overload the server, give it time before asking for more
    if next:
      sleep(1)

<% if (fileType === "json") { %>
  #json footer
  sys.stdout.write("\n] }\n")
<% } %>


if __name__ == "__main__":
  output_list()
